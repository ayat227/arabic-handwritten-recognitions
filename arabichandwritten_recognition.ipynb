{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a26f95f0",
   "metadata": {},
   "source": [
    "# Arabic Handwritten Recognition\n",
    "The automatic recognition of text on scanned images has enabled many applications such as searching for words in large volumes of documents, automatic sorting of postal mail, and convenient editing of previously printed documents. The domain of handwriting in the Arabic script presents unique technical challenges and has been addressed more recently than other domains. Many different methods have been proposed and applied to various types of images. Here we will focus on the recognition part of handwritten Arabic letters and digits recognition that face several challenges, including the unlimited variation in human handwriting and the large public databases.\n",
    "\n",
    "In this project we built a model which can classify a new image to an arabic letter or digit and get accuracy of 98.86% when testing on more than 13000 different images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20dc5060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import main libraries necessary for this project\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "\n",
    "# Import libraries needed for reading image and processing it\n",
    "import csv\n",
    "from PIL import Image\n",
    "from scipy.ndimage import rotate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfffd3d",
   "metadata": {},
   "source": [
    "the datse set i download was in zip file so at first we need to un zipped the file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cd8864e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'unzip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'unzip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "#The zipped file name is dataset.zip\n",
    "!unzip \"Arabic Handwritten Characters Dataset CSV-20230115T042302Z-001.zip\" -d \"datasetcharacter\"\n",
    "!unzip \"Arabic Handwritten Digits Dataset CSV-20230115T042309Z-001.zip\" -d \"datasetdigits\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb47842",
   "metadata": {},
   "source": [
    "we need to extract all the files from the zip file to dataset character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ac9e379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(\"Arabic Handwritten Characters Dataset CSV-20230115T042302Z-001.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"datasetcharacter\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e38fd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(\"Arabic Handwritten Digits Dataset CSV-20230115T042309Z-001.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"datasetdigits\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef2dccc",
   "metadata": {},
   "source": [
    "# Loading Arabic Letters Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ac42aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13440 training arabic letter images of 64x64 pixels.\n",
      "There are 3360 testing arabic letter images of 64x64 pixels.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>4086</th>\n",
       "      <th>4087</th>\n",
       "      <th>4088</th>\n",
       "      <th>4089</th>\n",
       "      <th>4090</th>\n",
       "      <th>4091</th>\n",
       "      <th>4092</th>\n",
       "      <th>4093</th>\n",
       "      <th>4094</th>\n",
       "      <th>4095</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4096 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1     2     3     4     5     6     7     8     9     ...  4086  \\\n",
       "0     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "1     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "2     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "3     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "4     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "\n",
       "   4087  4088  4089  4090  4091  4092  4093  4094  4095  \n",
       "0     0     0     0     0     0     0     0     0     0  \n",
       "1     0     0     0     0     0     0     0     0     0  \n",
       "2     0     0     0     0     0     0     0     0     0  \n",
       "3     0     0     0     0     0     0     0     0     0  \n",
       "4     0     0     0     0     0     0     0     0     0  \n",
       "\n",
       "[5 rows x 4096 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training letters images and labels files\n",
    "letters_training_images_file_path = \"datasetcharacter/Arabic Handwritten Characters Dataset CSV/training images.zip\"\n",
    "letters_training_labels_file_path = \"datasetcharacter/Arabic Handwritten Characters Dataset CSV/training labels.zip\"\n",
    "# Testing letters images and labels files\n",
    "letters_testing_images_file_path = \"datasetcharacter/Arabic Handwritten Characters Dataset CSV/testing images.zip\"\n",
    "letters_testing_labels_file_path = \"datasetcharacter/Arabic Handwritten Characters Dataset CSV/testing labels.zip\"\n",
    "\n",
    "# Loading dataset into dataframes\n",
    "training_letters_images = pd.read_csv(letters_training_images_file_path, compression='zip', header=None)\n",
    "training_letters_labels = pd.read_csv(letters_training_labels_file_path, compression='zip', header=None)\n",
    "testing_letters_images = pd.read_csv(letters_testing_images_file_path, compression='zip', header=None)\n",
    "testing_letters_labels = pd.read_csv(letters_testing_labels_file_path, compression='zip', header=None)\n",
    "\n",
    "# print statistics about the dataset\n",
    "print(\"There are %d training arabic letter images of 64x64 pixels.\" %training_letters_images.shape[0])\n",
    "print(\"There are %d testing arabic letter images of 64x64 pixels.\" %testing_letters_images.shape[0])\n",
    "training_letters_images.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e517217f",
   "metadata": {},
   "source": [
    "# loading the arabic numbers  dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dbb34fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 60000 training arabic digit images of 64x64 pixels.\n",
      "There are 10000 testing arabic digit images of 64x64 pixels.\n"
     ]
    }
   ],
   "source": [
    "# Training digits images and labels files\n",
    "digits_training_images_file_path = \"datasetdigits/Arabic Handwritten Digits Dataset CSV/training images.zip\"\n",
    "digits_training_labels_file_path = \"datasetdigits/Arabic Handwritten Digits Dataset CSV/training labels.zip\"\n",
    "# Testing digits images and labels files\n",
    "digits_testing_images_file_path = \"datasetdigits/Arabic Handwritten Digits Dataset CSV/testing images.zip\"\n",
    "digits_testing_labels_file_path = \"datasetdigits/Arabic Handwritten Digits Dataset CSV/testing labels.zip\"\n",
    "\n",
    "# Loading dataset into dataframes\n",
    "training_digits_images = pd.read_csv(digits_training_images_file_path, compression='zip', header=None)\n",
    "training_digits_labels = pd.read_csv(digits_training_labels_file_path, compression='zip', header=None)\n",
    "testing_digits_images = pd.read_csv(digits_testing_images_file_path, compression='zip', header=None)\n",
    "testing_digits_labels = pd.read_csv(digits_testing_labels_file_path, compression='zip', header=None)\n",
    "\n",
    "# print statistics about the dataset\n",
    "print(\"There are %d training arabic digit images of 64x64 pixels.\" %training_digits_images.shape[0])\n",
    "print(\"There are %d testing arabic digit images of 64x64 pixels.\" %testing_digits_images.shape[0])\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afabcff",
   "metadata": {},
   "source": [
    "So, let’s visualize some images to understand the inputs we will deal with in this model which are Arabic characters and numbers as the figure below show that , but first we need to convert the csv input to image "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03397c7c",
   "metadata": {},
   "source": [
    "# Convert csv values to an image\n",
    "Writting a method to be used later if we want visualization of an image from its pixels values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0dbe740c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_values_to_image(image_values, display=False):\n",
    "  image_array = np.asarray(image_values)\n",
    "  image_array = image_array.reshape(64, 64).astype('uint8')\n",
    "  # The original dataset is reflected so we will flip it then rotate for a better view only.\n",
    "  image_array = np.flip(image_array, 0)\n",
    "  image_array = rotate(image_array, -90)\n",
    "  new_image = Image.fromarray(image_array)\n",
    "  if display == True:\n",
    "    new_image.show()\n",
    "  return new_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225e9d58",
   "metadata": {},
   "source": [
    "# Visualizing some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b9f4a96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAAAAACPAi4CAAADH0lEQVR4nO3XSW/TQBQA4PGa2HHqJM7SpEmXqKiUsIiegAPiygGJH8wJCbVCKgWJFpoU2ibOQu0sju3xOjYH4BRP4/hWiXeylJkvb57H42cA7nwQS35nBS7tzyw/wA2gb59PZvcbNfNjT3OTACTLS42D7fWb7thMBLCF5ovWfoE6l3gSOwgP0OnSXuvFthjcDOcutgS3AFz90evWut+5+NK5GDurA4Rw/+CgZsonp9/VYI13obcaQJCll8834OGnoxGVX2fN8YW2YgaAEfMpy0V8tVSuM9qlYgThKkAYwMFAyj6sPKPLWc4bf+6oDlotA2869YSaFKZEypx6Nu4+4IuIIPSZXAAAmFy2j89kJ5qIBihWKDbrOQYwATIG8rfOZ1mLXAAOYEu7r1r3yoLnufbPd1+vVM3G7eUogKSlB0+f7+YC2bDovD26+jHDzMYAdLb55mCLs67fX5mNt4V6X8bPjwQIMp0XfaV7dnhtQXN9Rzmf+JF7AAcge9wh0upR+1onZkbqMfpyg9vI0UDgTk6nrHos6z4zHzWK25vD/kpA6P36kCIdzUKhNz3L5SstbW6uBIQQ/rvULypPss3RKbnyTvwbxrmkFXZVkQIYAX9W/QlHGU384kZZxP3TMgDByWDM15oNLiEQeuZQBbnNejopEJjdrs3v7PAJARDCXs9ga3WBjn6JLQWA2TnX6I2tAkclBBxlcAOFUhFzH5YDSFd+yCBfq0SXcTkQePP+EHG5xBkAYPdki+L5pDUAwB4NHbZUYpMDsmwLe3tCYsDXZgYoV7NM1OA4AHKNKcwUpbXIEzgGELqmOmNEKcckBAJH642CtWollTSDAPb6XqZaTQoAYLTbBuaJjAe4iuJidkI8AEGIGFFMWkQAkG1CPyNmUovD4wL62OAK0tpiDvGA0DGnOp0RIhrOeEDgmpM5waUjlrDsxfI3A4B8RFA0tXguxsuAIGguTREkmRQAAARhCIiIgznuElxd90EY0WbEA0Bo939mlKm12KrFBIB2MuvqbXWxV4sLWLI1c0ZwMYNlH13/gmQZDhkuttf6H3c7fgNKKGolip6lCAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=64x64>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_values_to_image(training_letters_images.loc[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ce43749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAAAAACPAi4CAAADBUlEQVR4nO2W308TQRDHd29v935L2wNasQiCKNKgBhI1xpj45p/rX2CMr0aTakQg+GBb2l4LHC3c7f3c9UEp2N5a4E3Tebub3c/OzXx35gCY2MQmNrH/xuDVdyAk8zA9e5SvDEC6biYd//oApVicp5+cmF0XYFUeP3H9oBdeFyAphXund5rBb4B0ZUBQr4fG0pIORBEgrFr8NIiYABA67f7MbEkRAog9t8q2mm4oiqBdc4ypaSIEaAurG0DRt2POMwGp1z/2sYqEgNzmk3V57qbbT7IBACSelw4EOApQy4vFBEMo1igLgvMEjQJkSwv3331wYlEAgMUxF0YAIdax19rZOxWkAAAgYXzuG9aBpOgGOmq2DiPhfoBzOTT4hpEIiKLh0PMC4XYoq6Ya0bPrOBwBUg2NQJGIAABA1i0DHnTOZDIM4CxNgaLJkrAI2J62QNcRAZL+kRuYBYMIL4m+sGSDxg8qALCY9qhhz+ax4Pz8/MOVtNPqiiIALDjs67fuL+vDjl9mrjx7/dj5XD+Of78YFVJQv71UqARJ/TRMOeec/zqFQyTJilneeFRmu1+6gyqNAnpVY23x5Xzp61aXxmmScCghifMUaZpeqqxt2nT7bdUFYgCt2XuWrbJ83vHDKKIJQCpiLMZ50yo+uD0TfPu4vR8Plo9WC5GFV09fFD2/7/sh9Y9CSEycxlSdn1KJQus773ebF3Q+GkFKO1Wg381rBguiMDhJkXoD89iXC5jSbuvLVrWVXBBall6wNbO8XClZcsqYhFVT0wni1HfbO41m13Hd8KJQs7pyfOT1neOShTkA2DRMTcWER7392rdGu+fHfy7OVqxEVFNFEgAAYaIQTAiIfPfkhAbJcJ8aNxshQgRjGfI48pOsOzZusHAGGZdSL+YszewQ46ezcdO2w++HNMl2jx9theePKsdvqo2TbPf40Sbnyuvr5ZzopPGAqNMNJW0wSEb4YwF0f3uOtT1BCi6RRDJlL/DaQT+6LgBCYgI/yi7ipX6yIJRByv/WqCf2z9tP/rRPE4V1mzIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=64x64>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_values_to_image(training_letters_images.loc[12], True)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fd765ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAAAAACPAi4CAAAEKElEQVR4nO3W2W/iRhgA8BmPbQw+uAMkhFy7be6FhERq1Updrbqt9o/tc9VuL62qthug5NoSAiEpEO7LDgYfY/pQqdqCwTz0qcr3hOD7fvPNmPEMAI/xGP+LgBY/I4okEU1SBBgZqqSoxngGaQHYvALHuwUnDfCwmal0hvMCBKIZxm6jhaCL491OwQbwsO66zjQ0PN8UKNazvBjxu90LLEXSJEmAkSE3zr+6aI31YNoBpDn3UmhtKeIVGBrqhq5iknc4YGCUqklzAJBwbm3FVxc4O1TFXEuUFWXAb61tOO3uSLEiWgMku3QQi/pGavuhXb+pi7KiaaGFRWNkGHg0Gk82AZilvc93BemycFcpdyVFwwbFeVw+Wu9V8reDOQCS80eW1FrhLFOq9/EIIMYbXg9yei9/cdtU5wAABFCT6n8WbpoKHgHAhHZfPVux9bJfp7M9fQ5Af2jcYLFQbfX6AECKDW4fxldw7er05Loz3oApMCwT9oDeqw4NAADknu692A7j3Pl3mYqkTc7XrAPpPuUklJ6oA8h4IvFoNKCWUolUUZvYCVP+iZBkSWjommIQoY9iH695+ue/v75qyRPPcEoHYKR1//5g8zw5jO/w4m0ycVkxGX4a8E84Y0efPhV6qeQ3122T0a0AmluNHawLnXwicVWdljQL4Hfiz7dc3bfJH3PdqUnTAZpbPTj8wC3mk4lsbUr/MwFu5+DVpk9KJX7Ids2W3wKgHMuxo02/dJNM5Orm6z8bYDcOv9z1SmeJb6+604efCiB7cP9gM6BW04nrqes/C2AW976I+pT785/Sndn15q800r0Z3Q7DxrvTbAWbJFgBpCPy4jgEpfzrk4YxcwFMAUjwa3u765x898fFzcOsBzAFIJjwy+MnAmz8/OudaDWBSQBCPrx/uOPWq+9OM53J94c1QIdeHu0HUfn7k5OSbF0/Adh8G7FnQbKZS6bLommFBcDvxrfD9mbyt1RunvHHAYLy7uyFmIdi+rTYe+9rxJAENPBAn9xV/wYo18pR1NnPJt5cvlcPERcSGIjFe1G1ABDrCQdQJ3NWaBiAAARiKERRDOtZdjEQNzOl2sS8xgAHZ0eD8i9v2wYBCURxASfP+3zLfj+LCKN6knpjAUBIIAQgYsMLNEnRjDPk5gSfb9HDI02RezaKGK8fAwxN1YBj+fl6Bwgs4+A4J0NSlI2h4LBeKxQu8w8WAO63bj1+54cLEiE4GIZjaIgx1rpDuVMqF8rFxsQda+xkQlTks+NPFjHGgEQIIQKoPbnfKVXuK6W2PBioVtc8jBtpzKzaaRpBoGOM9UFT6reL1ftGQzbfmONnI8V517wc53LYDLkvKlhuScPhUBmoypSNOXm40k6Hg3U7aEPuSwoeioPJtmcDBIlIRCEC6Fg3RoY2ea96jMf47+MvAo3ZbSub6u4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=64x64>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_values_to_image(training_letters_images.loc[40], True)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2eb5258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAAAAACPAi4CAAACpUlEQVR4nO3U20/aUBwH8HNOr5QCBVrEqqhUky1zc+5pyR62F//pvW3JlizLzK4mIshFUDNawV6ht7MHmRlFmm7ZY79vbc7v01/76zkApEmTJk2aNP8xMPlSRGWh5YV4/i6ZvJ4sKuhsNA3+GchUDhiuMzATAwiEf15SgnIoKUevEwIEzZehZk0CACHEIAQAMUJtfapxkYXLAFrceYXeNy8sACmIQ3z76cgsTyQEKKH2jHWgFRA5mXXMn7YPCQIiikJJgWJVlkJeAznlUFKbb7sWYphocRwQOPpI2PQvZH7neUWVXElllC3OG4+9hIB1isvG/lPRJPLrXKlav+zC+oY0ajQiQ1gKeOPeEb1ZfxAGvms5uJRhgZxV28dndkIAgJtPgcKtstaocd4nlSJBqv1B4107aQcATIe95kqZNPtHJwNqWOFIrPc73VEYWbYAIIDB7dBD/URUYPfrm9MJbDEEwr5jG9H6KAAzZUJ33NsNY7erGhW6w0uM1aWNRgHxJfutr86AltDLy/U85eF7Ku86nuO46sGBzM9Ub3TV07iKmKeW10cAZkXZfySys/89DMYnzaC4VWMTA0RWkCQh+3vDhFan62bKJToxACAEVLF417LVahlEoZD8FQLbNIOSxKLZUeldX7uQZuIOznlgetW7sKubBXpWgr2FQzSa+TH6ltorlHYfo6HpBhjSbEGIa38RANj4wb9YzR1/6Gi2R5dFeU9AILaHKGCe5jY2agxc08wJJYlr24w51v3kADC+3+h7+/JaOHEMkOd5Bl62us5fAO61DwywW+MgQZMlBhnnX9rDaQywOCEym1+pPcmqQU7czpj9j43WyF3Yg3EAAAQjPmQ1XJRqjD74PDDdmAbuBSCic8jDFJ1BgaM7Qczz06T5T/kFOdsSrRQxipwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=64x64>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_values_to_image(training_letters_images.loc[3000], True)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c23ccd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAAAAACPAi4CAAACuUlEQVR4nO3WSXPTMBQAYMlOvCRu6mwOaZq0KWkZSodheugFfjZ/gRvLtDAsU9qSJilx4zjeI8e2xIEOcLDs2HBhpu9oPX+WPE9PAuA+7uM+/knA+MecIHea5fLPUeK5C80w3djMQjzAy/3nT9vtO0CdXry/uswElLsHh0fNBgQAAkDkukAYbZoFqB0+2+/wgBAIIIAVjuWYr/GZFKDSHzyQbC0iDM9EmGUBL7CZAGmnJ1hv36GVWBdcVCqZ37+pmYBypw3M9y9tr9KTdEuuWqo2ywSwxWIQunMHeT7n+Zro256fCWC4Yhg4uoPILcQEMgRjnAlAus6yAEcRDuMTfn8q/rE7vlkWCixDKdR0wNf1sChWBMrwH0FZgm9YkVDvEjfIOwPDCDl5S6b46TNwRi2HbTycF1MB2k+cjF2m1m/mBvzFbO5Ig25NSlsErQ40VbM2lE8tM0opBAoQIXMy3GluPQrwilBqMBHAxDqX+d72C9ExgzwAIe54cy+oPnY+T53ERVD/ERoXnszZ9t6+OYxvhndBrVVfvbyZg0a336skvU8HSOCNzkar4mZVyAfg0Budjn2ukhcAJLKnRliUa3kBgJ1bO+KqjRJM6gqJwExVXVFpKVJCVhJgjr68ueJ3j066CRsiYQi77vAD31EGCzQKCckOAABmZ/yuuHWyuIAerRyTAQ1Jx9ud7cvXaJUPCFz1Y2nQ6RyxS5QLCLF6CquD7nFwvaCkJPdtElnDc9XbGHQ3+Wyn869wr9nhTDiYKKoT5ZgBACtdnemwqdQrXHxC+tET2TOPkVotMTfgzJew3GrmBkJjYmGx8ReAPjawqCi5Abw0EYaFAiVzHcBChN4S1gB8G0UB8uPLYB3AM71guTAol6w1AGTpt6o6p+ym9BsEXoJXlj1Sjfjh9FsUgIBhCcHUnvTfxw8fpUfTBL/zMgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=64x64>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "convert_values_to_image(training_digits_images.loc[2], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3aecb457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAAAAACPAi4CAAAD6klEQVR4nO3W21PiVhgA8HNyExOBJBAgQBCt6Gjd1Yd27XRm//e2D33oOnbXsetlWUUJ1yRczN3k5PRl2wUBzfLQ6YPfE3wn+fF9X3LOAMBLvMT/IeC8FEWRFEEAhMIgivC3ApCm8wVJTnGkprXuxtbD0wA1kyFoVvl+Z7+UYS6v/ozwwzcBFJ3OZ7NiUU46HZ9NbJNrVx9U70liGqDZ0g87GwpF6s2ezpfXc1sntjWKDZBEtrxzUMsKeveubrMCElbL5q5TN8ETg5wAIMVUj/Ze5Vzj5FhtuyTvc3mpcpRwb2NWQKSErcMNEejq2XF34IFkSSFywqb3nopQLICpHR7uwfPWZb3VskMAgpvfQHktmUlzvre4hwmA3nhb21Kv/ji9wBgAAII7S37DrYlpDvuLAeLrR1Kqpe325YXx5WrsGD3VAMmcwpOxWiCztYdx6/Ii/AJEjttTeT6Vr7jdxcBEBZBaCXTN9MJ/EhHSzq/tlXxNouZsmVkAAGC1+97k997JX2OmtC8zcKEwAaCxqtmIXJlo2Gr3TCCsSxyzEJiYQVD/hSGSfNpyvqYc0/KSTF50o0Xv80QFoXrcZkS5kCQJCCEkCAiRZ48sKitlU7O7drYCNGqWSwV2/d2p72NI0CSKGNJuiNtr0i66d54HolHzTVFU1s2GhTCkVukwIAnnJilVpJ1RPUYFkQ2vfq8KuZ+SjhNhMgEat5Y16mQsmF6/ZckFZ9sU4J1RBz9X3h55XoQh4/+K74xhizehsFHnSDB/R00OB6H+OUhYIg9p1w5cZ2x5oTfURw4lSrnugilMT3cU3Pcqe1sAG617z6rfWGE07OuDRLpQHd7GAVzXGrY9FxL926Fj6zpCkTvW2jIvlLva3PtnTuXQjFADQmfsBYGLIgyQ3XgfSWtK/yYmEDpTP4WRq57lQq7Y4eDcx7DwDfs3HrpMbUDI1QLvuXOWiTm5R0DnvDkgitWCkJi3o54HsG/p6hCmlS1xHvB8C1EA+tdsLvXd0G4sVQGIQuNz22WK2/nEnJMpBgBw7/STAYq7ZYGdvfz5FgDAg7Bq5HlGziBvZkPEqsA3tWbHX1UOlJWZxTgA8O/1ZjdgS/slZjkA4MHHjzrgq9IK8XiM8QBgnH7QgLCZTywLuHrPMKlsrpB53ERMIDAH/T7OKNsV7tFKnMcIAECRpXVZNr/jDE009ShjAgB77c98Vf6RGnljcxkAeN2bTSRxUV0LlgPcO6qkpLjc6xD1lgTsykY1LR8QzdOlgMi/v37nrSLbD6fysQGM3E+WnyabXXs5AOBAd0WR6qjjqfTi/y4zAWlakqFjGcP497zEfxN/A/mm3s0rD6IQAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=64x64>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "convert_values_to_image(training_digits_images.loc[9], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79a7156",
   "metadata": {},
   "source": [
    " # Data Preprocessing\n",
    "# Image Normalization\n",
    "We rescale the images by dividing every pixel in the image by 255 to make them into range [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88f2eab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images of digits after scaling\n",
      "(60000, 4096)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_digits_images_scaled = training_digits_images.values.astype('float32')/255\n",
    "training_digits_labels = training_digits_labels.values.astype('int32')\n",
    "testing_digits_images_scaled = testing_digits_images.values.astype('float32')/255\n",
    "testing_digits_labels = testing_digits_labels.values.astype('int32')\n",
    "\n",
    "training_letters_images_scaled = training_letters_images.values.astype('float32')/255\n",
    "training_letters_labels = training_letters_labels.values.astype('int32')\n",
    "testing_letters_images_scaled = testing_letters_images.values.astype('float32')/255\n",
    "testing_letters_labels = testing_letters_labels.values.astype('int32')\n",
    "     \n",
    "\n",
    "print(\"Training images of digits after scaling\")\n",
    "print(training_digits_images_scaled.shape)\n",
    "training_digits_images_scaled[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90c447a",
   "metadata": {},
   "source": [
    "# Encoding Categorical Labels\n",
    "From the labels csv files we can see that labels are categorical values and it is a multi-class classification problem.\n",
    "\n",
    "Our outputs are in the form of:\n",
    "\n",
    "Digits from 0 to 9 have categories numbers from 0 to 9\n",
    "Letters from ’alef’ to ’yeh’ have categories numbers from 10 to 37\n",
    "Here we will encode these categories values using One Hot Encoding with keras.\n",
    "\n",
    "One-hot encoding transforms integer to a binary matrix where the array contains only one ‘1’ and the rest elements are ‘0’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87c29440",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# one hot encoding\n",
    "# number of classes = 10 (digits classes) + 28 (arabic alphabet classes)\n",
    "number_of_classes = 38\n",
    "training_letters_labels_encoded = to_categorical(training_letters_labels, num_classes=number_of_classes)\n",
    "testing_letters_labels_encoded = to_categorical(testing_letters_labels, num_classes=number_of_classes)\n",
    "training_digits_labels_encoded = to_categorical(training_digits_labels, num_classes=number_of_classes)\n",
    "testing_digits_labels_encoded = to_categorical(testing_digits_labels, num_classes=number_of_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7793c236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(training_digits_labels_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bd65a1",
   "metadata": {},
   "source": [
    "# Reshaping Input Images to 64x64x1\n",
    "When using TensorFlow as backend, Keras CNNs require a 4D array (which we'll also refer to as a 4D tensor) as input, with shape (nb_samples,rows,columns,channels)\n",
    "\n",
    "where nb_samples corresponds to the total number of images (or samples), and rows, columns, and channels correspond to the number of rows, columns, and channels for each image, respectively.\n",
    "\n",
    "So we will reshape the input images to a 4D tensor with shape (nb_samples, 64, 64 ,1) as we use grayscale images of 64x64 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6a132fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 64, 64, 1) (60000, 38) (10000, 64, 64, 1) (10000, 38)\n",
      "(13440, 64, 64, 1) (13440, 38) (3360, 64, 64, 1) (3360, 38)\n"
     ]
    }
   ],
   "source": [
    "# reshape input digit images to 64x64x1\n",
    "training_digits_images_scaled = training_digits_images_scaled.reshape([-1, 64, 64, 1])\n",
    "testing_digits_images_scaled = testing_digits_images_scaled.reshape([-1, 64, 64, 1])\n",
    "\n",
    "# reshape input letter images to 64x64x1\n",
    "training_letters_images_scaled = training_letters_images_scaled.reshape([-1, 64, 64, 1])\n",
    "testing_letters_images_scaled = testing_letters_images_scaled.reshape([-1, 64, 64, 1])\n",
    "\n",
    "print(training_digits_images_scaled.shape, training_digits_labels_encoded.shape, testing_digits_images_scaled.shape, testing_digits_labels_encoded.shape)\n",
    "print(training_letters_images_scaled.shape, training_letters_labels_encoded.shape, testing_letters_images_scaled.shape, testing_letters_labels_encoded.shape)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca1fe3d",
   "metadata": {},
   "source": [
    "# merging the letters and numbers \n",
    "And now, after the dataset are ready we will merging the dataset merging the letters and numbers together the dataset combining together with 73440 for training and 13360 for testing with 38 classes which are 0-9 for numbers and 10-37 for letters as mentioned below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09f9cdaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Training images are 73440 images of shape\n",
      "(73440, 64, 64, 1) (73440, 38)\n",
      "Total Testing images are 13360 images of shape\n",
      "(13360, 64, 64, 1) (13360, 38)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "training_data_images = np.concatenate((training_digits_images_scaled, training_letters_images_scaled), axis=0) \n",
    "training_data_labels = np.concatenate((training_digits_labels_encoded, training_letters_labels_encoded), axis=0)\n",
    "print(\"Total Training images are {} images of shape\".format(training_data_images.shape[0]))\n",
    "print(training_data_images.shape, training_data_labels.shape)\n",
    "\n",
    "\n",
    "testing_data_images = np.concatenate((testing_digits_images_scaled, testing_letters_images_scaled), axis=0) \n",
    "testing_data_labels = np.concatenate((testing_digits_labels_encoded, testing_letters_labels_encoded), axis=0)\n",
    "print(\"Total Testing images are {} images of shape\".format(testing_data_images.shape[0]))\n",
    "print(testing_data_images.shape, testing_data_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b155ed",
   "metadata": {},
   "source": [
    "# Designing Model Architecture\n",
    "Now we will make a method which creates the model architecture with the specified optimizer and activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35c6325f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, BatchNormalization, Dropout, Dense\n",
    "\n",
    "def create_model(optimizer='adam', kernel_initializer='he_normal', activation='relu'):\n",
    "  # create model\n",
    "  model = Sequential()\n",
    "  model.add(Conv2D(filters=16, kernel_size=3, padding='same', input_shape=(64, 64, 1), kernel_initializer=kernel_initializer, activation=activation))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(MaxPooling2D(pool_size=2))\n",
    "  model.add(Dropout(0.2))\n",
    "\n",
    "  model.add(Conv2D(filters=32, kernel_size=3, padding='same', kernel_initializer=kernel_initializer, activation=activation))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(MaxPooling2D(pool_size=2))\n",
    "  model.add(Dropout(0.2))\n",
    "\n",
    "  model.add(Conv2D(filters=64, kernel_size=3, padding='same', kernel_initializer=kernel_initializer, activation=activation))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(MaxPooling2D(pool_size=2))\n",
    "  model.add(Dropout(0.2))\n",
    "\n",
    "  model.add(Conv2D(filters=128, kernel_size=3, padding='same', kernel_initializer=kernel_initializer, activation=activation))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(MaxPooling2D(pool_size=2))\n",
    "  model.add(Dropout(0.2))\n",
    "  model.add(GlobalAveragePooling2D())\n",
    "  \n",
    "  #Fully connected final layer\n",
    "  model.add(Dense(38, activation='softmax'))\n",
    "\n",
    "  # Compile model\n",
    "  model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=optimizer)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f4e161",
   "metadata": {},
   "source": [
    "Let’s understand above code step by step.\n",
    "\n",
    "The first hidden layer is a convolutional layer. The layer has 16 feature maps, which with the size of 3×3 and an activation function which is relu. This is the input layer, expecting images with the structure outlined above.\n",
    "The second layer is Batch Normalization which solves having distributions of the features vary across the training and test data, which breaks the IID assumption. We use it to help in two ways faster learning and higher overall accuracy.\n",
    "The third layer is the MaxPooling layer. MaxPooling layer is used to down-sample the input to enable the model to make assumptions about the features so as to reduce overfitting. It also reduces the number of parameters to learn, reducing the training time.\n",
    "The next layer is a Regularization layer using dropout. It is configured to randomly exclude 20% of neurons in the layer in order to reduce overfitting.\n",
    "Another hidden layer with 32 feature maps with the size of 3×3 and a relu activation function to capture more features from the image.\n",
    "Other hidden layers with 64 and 128 feature maps with the size of 3×3 and a relu activation function to capture complex patterns from the image which will decribe the digits and letters later.\n",
    "More MaxPooling, Batch Normalization, Regularization and GlobalAveragePooling2D layers.\n",
    "The last layer is the output layer with 10 neurons (number of output classes) and it uses softmax activation function as we have multi-classes. Each neuron will give the probability of that class.\n",
    "I used categorical_crossentropy as a loss function because its a multi-class classification problem. I used accuracy as metrics to improve the performance of our neural network.\n",
    "\n",
    "# Model Summary And Visualization\n",
    "Let's see the model summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0df3e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 64, 64, 16)        160       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 64, 64, 16)       64        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 32, 32, 16)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32, 32, 16)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 32, 32, 32)        4640      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 32, 32, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 16, 16, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16, 16, 32)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 16, 16, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 16, 16, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 8, 8, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 8, 8, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 4, 4, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 128)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 38)                4902      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 103,014\n",
      "Trainable params: 102,534\n",
      "Non-trainable params: 480\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04e8dfc",
   "metadata": {},
   "source": [
    "# Parameters Tuning\n",
    "We will tune the parameters optimizer, kernel_initializer and activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68b88329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of different parameter combinations = 24\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# define the grid search parameters\n",
    "optimizer = ['RMSprop', 'Adam', 'Adagrad', 'Nadam']\n",
    "kernel_initializer = ['normal', 'uniform']\n",
    "activation = ['relu', 'linear', 'tanh']\n",
    "\n",
    "param_grid = dict(optimizer=optimizer, kernel_initializer=kernel_initializer, activation=activation)\n",
    "\n",
    "# count number of different parameters values combinations\n",
    "parameters_number = 1\n",
    "for x in param_grid:\n",
    "  parameters_number = parameters_number * len(param_grid[x]) \n",
    "print(\"Number of different parameter combinations = {}\".format(parameters_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2328d9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'optimizer': 'RMSprop', 'kernel_initializer': 'normal', 'activation': 'relu'}\n",
      "Epoch 1/5\n",
      "3672/3672 [==============================] - 310s 84ms/step - loss: 0.3394 - accuracy: 0.9060 - val_loss: 5.2711 - val_accuracy: 0.1708\n",
      "Epoch 2/5\n",
      "3672/3672 [==============================] - 310s 85ms/step - loss: 0.1156 - accuracy: 0.9642 - val_loss: 0.1317 - val_accuracy: 0.9588\n",
      "Epoch 3/5\n",
      " 959/3672 [======>.......................] - ETA: 3:34 - loss: 0.0969 - accuracy: 0.9703"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "batch_size = 20 # 20 divides the training data samples\n",
    "\n",
    "#creating the models with different hyperparameters\n",
    "for a,b,c in [(x,y,z) for x in optimizer for z in activation for y in kernel_initializer]:\n",
    "    params = {'optimizer' : a , 'kernel_initializer' : b , 'activation' : c}\n",
    "    print(params)\n",
    "    curr_model = create_model(a, b, c)\n",
    "    curr_model.fit(training_data_images, training_data_labels, \n",
    "                    validation_data=(testing_data_images, testing_data_labels),\n",
    "                    epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "    print(\"=============================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5072d2d0",
   "metadata": {},
   "source": [
    "From the above results we can see that best parameters are:\n",
    "\n",
    "Optimizer: Adam\n",
    "Kernel_initializer: uniform\n",
    "Activation: relu\n",
    "Let's create the model with the best parameters obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45715049",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(optimizer='Adam', kernel_initializer='uniform', activation='relu')\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5303e281",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "# Fitting the Model\n",
    "Train the model using batch_size=20 to reduce used memory and make the training more quick. We will train the model first on 10 epochs to see the accuracy that we will obtain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599eca07",
   "metadata": {},
   "source": [
    "Model Evaluation and Validation\n",
    "We will Train the model using batch_size=20 to reduce used memory and make the\n",
    "training more quick. We will train the model first on 10 epochs to see the accuracy\n",
    "that we will obtain then we will increase number of epochs to be trained on to\n",
    "improve the accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd806afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint  \n",
    "\n",
    "# using checkpoints to save model weights to be used later instead of training again on the same epochs.\n",
    "checkpointer = ModelCheckpoint(filepath='weights.hdf5', verbose=1, save_best_only=True)\n",
    "history = model.fit(training_data_images, training_data_labels, \n",
    "                    validation_data=(testing_data_images, testing_data_labels),\n",
    "                    epochs=10, batch_size=20, verbose=1, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da5e1e6",
   "metadata": {},
   "source": [
    "# Plotting Loss and Accuracy Curves with Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28c44f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_loss_accuracy(history):\n",
    "  # Loss Curves\n",
    "  plt.figure(figsize=[8,6])\n",
    "  plt.plot(history.history['loss'],'r',linewidth=3.0)\n",
    "  plt.plot(history.history['val_loss'],'b',linewidth=3.0)\n",
    "  plt.legend(['Training loss', 'Validation Loss'],fontsize=18)\n",
    "  plt.xlabel('Epochs ',fontsize=16)\n",
    "  plt.ylabel('Loss',fontsize=16)\n",
    "  plt.title('Loss Curves',fontsize=16)\n",
    "\n",
    "  # Accuracy Curves\n",
    "  plt.figure(figsize=[8,6])\n",
    "  plt.plot(history.history['acc'],'r',linewidth=3.0)\n",
    "  plt.plot(history.history['val_acc'],'b',linewidth=3.0)\n",
    "  plt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=18)\n",
    "  plt.xlabel('Epochs ',fontsize=16)\n",
    "  plt.ylabel('Accuracy',fontsize=16)\n",
    "  plt.title('Accuracy Curves',fontsize=16) \n",
    "     \n",
    "\n",
    "plot_loss_accuracy(history)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a098b24e",
   "metadata": {},
   "source": [
    "# Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7937133",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('weights.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a145d7e",
   "metadata": {},
   "source": [
    "# Test the Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5764f8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation of the model\n",
    "metrics = model.evaluate(testing_data_images, testing_data_labels, verbose=1)\n",
    "print(\"Test Accuracy: {}\".format(metrics[1]))\n",
    "print(\"Test Loss: {}\".format(metrics[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498808cf",
   "metadata": {},
   "source": [
    "# now lets train the model by increasing the number of epochs and after that check the overviting using plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e7773f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Training More on the best model\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 20\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='weights.hdf5', verbose=1, save_best_only=True)\n",
    "\n",
    "history = model.fit(training_data_images, training_data_labels, \n",
    "                    validation_data=(testing_data_images, testing_data_labels),\n",
    "                    epochs=epochs, batch_size=batch_size, verbose=1, callbacks=[checkpointer])\n",
    "          \n",
    "model.load_weights('weights.hdf5')\n",
    "plot_loss_accuracy(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7238b1ef",
   "metadata": {},
   "source": [
    "# A Demo to see how the model works on examples of the testing data\n",
    "Let's recall:\n",
    "\n",
    "Digits from 0 to 9 were encoded to categorical labels from 0 to 9\n",
    "Letters from ’alef’='أ' to ’yeh’='ى' were encoded to categorical labels from 10 to 37\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6baf986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_classes(model, data, labels=None):\n",
    "  image_predictions = model.predict(data)\n",
    "  predicted_classes = np.argmax(image_predictions, axis=1)\n",
    "  true_classes = np.argmax(labels, axis=1)\n",
    "  return predicted_classes, true_classes\n",
    "\n",
    "def get_non_zero_index_from_one_hot_encoding(one_hot_encoding):\n",
    "  non_zero_index = np.where(one_hot_encoding == 1)[0]\n",
    "  assert(len(non_zero_index) == 1)\n",
    "  non_zero_index = non_zero_index[0]\n",
    "  return non_zero_index\n",
    "     \n",
    "\n",
    "def convert_categorical_label_to_real_label(categorical_label):\n",
    "  real_labels = []\n",
    "  real_labels.extend([x for x in range(10)])\n",
    "  real_labels.extend(['أ', 'ب', 'ت', 'ث', 'ج', 'ح', 'خ', 'د', 'ذ', 'ر', 'ز', 'س', 'ش', 'ص', 'ض', 'ط', 'ظ', 'ع', 'غ', 'ف', 'ق', 'ك', 'ل', 'م', 'ن', 'ه', 'و', 'ى'])\n",
    "  return real_labels[categorical_label]\n",
    "     \n",
    "\n",
    "def show_example(sample_index):\n",
    "  y_pred, y_true = get_predicted_classes(model, testing_data_images[[sample_index]], testing_data_labels[[sample_index]])\n",
    "  non_zero_index = get_non_zero_index_from_one_hot_encoding(testing_data_labels[sample_index])\n",
    "  y_true = y_true[0]\n",
    "  y_pred = y_pred[0]\n",
    "  assert y_true == non_zero_index\n",
    "  true_label = convert_categorical_label_to_real_label(y_true)\n",
    "  predicted_label = convert_categorical_label_to_real_label(y_pred)\n",
    "  print(\"The following image has the written character '{}' but the model predicted it as '{}'\".format(true_label, predicted_label))\n",
    "  return true_label == predicted_label\n",
    "     \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4c8a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = 5\n",
    "show_example(sample_index)\n",
    "convert_values_to_image(testing_digits_images.loc[sample_index], True)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c32acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = 11414\n",
    "show_example(sample_index)\n",
    "convert_values_to_image(testing_letters_images.loc[sample_index - testing_digits_images.shape[0]], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19d59f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f15498",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
